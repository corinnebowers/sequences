---
title: "create_df"
output: html_document
date: "2022-09-27"
author: Corinne Bowers
theme: spacelab   #https://www.datadreaming.org/post/r-markdown-theme-gallery/
highlight: tango  #https://www.garrickadenbuie.com/blog/pandoc-syntax-highlighting-examples/
---

This script creates $df_3hr$ and $df_24hr$, datasets of hazard and impact for all MERRA-2 grid cells in California at temporal resolutions of 3 hours and 24 hours, respectively.

```{r setup, include = FALSE}
knitr::opts_knit$set(root.dir = 'D:/2-sequences/')
# knitr::opts_knit$set(root.dir = '/scratch/users/cbowers/sequences/')
# knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
knitr::opts_chunk$set(results = 'hold', fig.show = 'hold', fig.align = 'center')
# rm(list=ls())

```

# setup 

```{r}
## load packages & functions
source('_data/setup.R')
source('_scripts/create_df_functions.R')
# cores <- parallel::detectCores()-2

mode <- function(x) ifelse(all(is.na(x)), NA, names(which.max(table(x)))[1])

```

# create 3-hr dataframe

## load data

### MERRA metadata

```{r}
## get hourly/daily timeseries
ts_merra <- 
  seq(ymd_hms('1980-1-1 00:00:00'), ymd_hms('2021-12-31 21:00:00'), by = '3 hours')
dates_merra <- ts_merra %>% as.Date %>% unique

```

### IVT

```{r}
## load MERRA IVT
load('_data/MERRA/ivt/ivt_merra_0928.Rdata')  # ivt_merra, ts_merra

## crop to California
ivt_ca <- ivt_merra %>% crop(grid_ca)
rm(ivt_merra)

```

### AR

```{r}
## load MERRA IVT
load('_data/MERRA/ivt/ar_merra_0928.Rdata')  # ar_merra, ts_merra

## crop to California
ar_ca <- ar_merra %>% crop(grid_ca)
rm(ar_merra)

```

### precipitation

```{r}
## load MERRA precipitation
load('_data/MERRA/precip/precip_3hr_0922.Rdata')
precip_ca <- prcp
rm(prcp)

```

### soil moisture

```{r}
## load WLDAS soil moisture
load('_data/WLDAS/sm_stack_1109.Rdata')

```

## combine into dataframe

```{r}
# takes about 30 minutes on local machine

start <- Sys.time()
pb <- txtProgressBar(min = 0, max = ncell(grid_ca), style = 3)
cl <- makeCluster(cores)
registerDoSNOW(cl)
data_3hr <-
  foreach(
    i = 1:ncell(grid_ca),
    .options.snow = opts,
    # .export = c('wateryear', 'positive', 'lag', 'setNA'),
    .packages = c('raster', 'foreach', 'tidyverse', 'lubridate')) %dopar% {
      if (i %in% index_ca) {
        ## create dataframe for the specified cell
        index <- rowColFromCell(ar_ca, i)
        data <-
          data.frame(
            ts = ts_merra,
            ivt = c(ivt_ca[index[1],index[2],]),
            ar = c(ar_ca[index[1],index[2],])==1,
            precip = c(precip_ca[index[1],index[2],])) %>%
          mutate(ar.count = add_counter(ar))

        ## add soil moisture data
        data.sm <-
          data.frame(
            date = dates_merra,
            sm = c(sm_stack[index[1],index[2],]))
        data <- data %>%
          mutate(date = as.Date(ts_merra)) %>%
          left_join(data.sm, by = 'date') %>%
          select(-date)

        ## return dataframe
        data
      } else NULL
    }
stopCluster(cl)
Sys.time() - start

```

```{r}
## save out
save(data_3hr, file = '_scripts/_checkpoints/df_3hr_1209.Rdata')

# ## load from file
# load('_scripts/_checkpoints/data_3hr_1209.Rdata')

```

```{r}
start <- Sys.time()
pb <- txtProgressBar(min = 0, max = ncell(grid_ca), style = 3)
cl <- makeCluster(cores)
registerDoSNOW(cl)
df_3hr <-
  foreach(
    i = 1:ncell(grid_ca),
    .options.snow = opts,
    .export =
      c('wateryear', 'positive', 'lag', 'setNA', 'add_counter',
        'create_catalog', 'assign_AR_cat'),
    .packages = c('raster', 'foreach', 'tidyverse', 'lubridate')) %dopar% {
      if (i %in% index_ca) {
        ## attach AR catalog information to dataframe
        data <- data_3hr[[i]]
        catalog <-
          create_catalog(data, name = 'ar', interval = 3) %>%
          filter(duration > 6) %>%
          mutate(start = paste(start), end = paste(end))
        data <- data %>%
          left_join(
            catalog %>% setNames(paste('ar', names(.), sep = '.')),
            by = 'ar.count') %>%
          mutate(
            ar = ifelse(!is.na(ar.duration), TRUE, FALSE),
            ar.count = ifelse(!is.na(ar.duration), ar.count, NA))

        ## attach sequence information to dataframe
        startend <- data %>%
          filter(wateryear(ts) %in% 1981:2010) %>%
          pull(ivt) %>% median(na.rm = TRUE)
        data <- data %>%
          mutate(
            rolling5 = lag(ivt, 5*8, 'mean', 'center') %>% setNA(.,0),
            seq = rolling5 > startend) %>% 
        #   mutate(gap.count = add_counter(!seq))
        # ## combine sequences with gaps of <= 6 hours
        # override <- data %>%
        #   filter(!seq) %>%
        #   group_by(gap.count) %>%
        #   summarize(dur = length(gap.count)) %>%
        #   filter(dur <= 2) %>% pull(gap.count)
        # data <- data %>%
        #   mutate(seq = ifelse(gap.count %in% override, TRUE, seq)) %>% 
        #   select(-gap.count) %>%
          mutate(seq.count = add_counter(seq))
        sequences <- data %>%
          select(-ivt) %>% rename(ivt = rolling5) %>%
          create_catalog(., name = 'seq', interval = 3/24, cat = FALSE) %>%
          rename(maxrolling = maxivt) %>%
          filter(maxrolling > 250) %>%
          filter(month(start) %in% c(10:12,1:3)) %>%
          mutate(start = paste(start), end = paste(end))
        data <- data %>%
          left_join(
            sequences %>% setNames(paste('seq', names(.), sep = '.')),
            by = 'seq.count') %>%
          mutate(
            seq = ifelse(!is.na(seq.duration), TRUE, FALSE),
            seq.count = ifelse(!is.na(seq.duration), seq.count, NA))

        ## return dataframe
        data
      } else NULL
    }
stopCluster(cl)
Sys.time() - start

```

## checkpoint

```{r}
## save out
save(df_3hr, file = '_scripts/_checkpoints/df_3hr_1209.Rdata')

# ## load from file
# load('_scripts/_checkpoints/df_3hr_1209.Rdata')

```

# create daily dataframe

## runoff

```{r}
## load gridded runoff 
load('_data/streamflow/runoff_0930.Rdata')

```

## normalized flow

```{r}
## load gridded normalized streamflow
load('_data/streamflow/norm_rp2_1002.Rdata')

```

## combine with impacts

```{r}
# takes about 15 minutes on local machine

start <- Sys.time()
pb <- txtProgressBar(min = 0, max = ncell(grid_ca), style = 3)
cl <- makeCluster(cores)
registerDoSNOW(cl)
df_24hr <- 
  foreach(
    i = 1:ncell(grid_ca),
    .options.snow = opts,
    .export = c('attach_impacts', 'mode'),
    .packages = c('raster', 'tidyverse', 'lubridate')) %dopar% {
      if (i %in% index_ca) {
        ## aggregate 3-hour data
        data <-
          df_3hr[[i]] %>% 
          group_by(date = as.Date(ts)) %>% 
          summarize(
            ar = mean(ar) >= 0.25,
            ar.count = case_when(ar ~ mode(ar.count)),
            seq.3hr = mean(seq) >= 0.25,
            # seq.count = case_when(seq ~ mode(seq.count)),
            ivt.snap = sample(ivt,1),
            ivt.max = Max(ivt),
            precip = Sum(precip),
            sm = sm[1])
        
        ## attach sequences
        startend <- data %>% 
          filter(wateryear(date) %in% 1981:2010) %>% 
          pull(ivt.snap) %>% median
        data <- data %>% 
          rename(ivt = ivt.snap) %>% 
          mutate(rolling5 = lag(ivt, 5, 'mean', 'center') %>% setNA(.,0)) %>% 
          mutate(seq = rolling5>startend) %>% 
          mutate(seq.count = add_counter(seq))
        sequences <- calculate_sequences(data, data) %>%
          filter(month(start_date) %in% c(10:12,1:3)) %>% 
          select(-start_date)
        data <- data %>% 
          left_join(
            sequences %>% setNames(paste('seq', names(.), sep = '.')),
            by = 'seq.count') %>% 
          mutate(
            seq = ifelse(!is.na(seq.duration), TRUE, FALSE),
            seq.count = ifelse(!is.na(seq.duration), seq.count, NA)) %>% 
          rename(ivt.snap = ivt)

        ## attach runoff
        if (is.null(runoff[[i]])) {
          data$runoff <- NA
        } else {
          data <- data %>% left_join(runoff[[i]] %>% select(date, runoff), by = 'date')
        }

        ## attach normalized streamflow
        if (is.null(norm_rp2[[i]])) {
          data$rp2 <- NA
          data$norm_rp2 <- NA
        } else {
          data <- data %>% left_join(norm_rp2[[i]] %>% select(date, norm_rp2), by = 'date')
        }
        
        ## load impacts data
        load('_data/impacts/disasters_0922.Rdata')    # FEMA disaster declarations
        load('_data/impacts/pa_0922.Rdata')           # FEMA public assistance
        load('_data/impacts/ncei_0922.Rdata')         # NCEI storm events database
        load('_data/impacts/NFIP_0922.Rdata')         # NFIP claims & policies
        load('_data/impacts/poprasters_0922.Rdata')   # gridded population
        load('_data/impacts/wwa_0922.Rdata')          # NWS watches, warnings, & advisories
  
        ## attach impacts
        attach_impacts(data, i)
        
      } else NULL
    }
stopCluster(cl)
Sys.time() - start

```

## checkpoint

```{r}
## save out
save(df_24hr, file = '_scripts/_checkpoints/data_24hr_1209.Rdata') 
 
# ## load from file
# load('_scripts/_checkpoints/data_24hr_1209.Rdata')

```

