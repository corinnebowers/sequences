---
title: "get_impacts"
output: html_document
author: Corinne Bowers
date: '2022-06-16'
theme: spacelab   #https://www.datadreaming.org/post/r-markdown-theme-gallery/
highlight: tango  #https://www.garrickadenbuie.com/blog/pandoc-syntax-highlighting-examples/
---

This script downloads & processes the impacts data included in $df_3hr$ and $df_24hr$, including:

* FEMA disaster declarations,
* FEMA public assistance records,
* NCEI storm events database,
* NFIP claims & policies,
* gridded population, and
* NWS watches, warnings, & advisories.

```{r setup, include = FALSE}
knitr::opts_knit$set(root.dir = 'D:/2-sequences/')
# knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
knitr::opts_chunk$set(results = 'hold', fig.show = 'hold', fig.align = 'center')
# rm(list=ls())

```

# setup 

```{r}
## get functions & packages
source('_data/setup.R')
source('_scripts/create_df_functions.R')

## set up parallel backend
cores <- parallel::detectCores()-2

```

## define inflation adjustment

```{r}
## load inflation rates relative to Jan 2022
bls <- download_bls()
inflation <- 
  expand.grid(yr = 1970:2021, mo = 1:12) %>%
  mutate(
    inflation = map2_dbl(
      .x = yr, .y = mo,
      .f = ~calculate_inflation(yr=.x, mo=.y, ref_yr=2022, ref_mo=1, bls=bls)))

```

## define MERRA metadata 

```{r}
## get hourly/daily timeseries
ts_merra <- seq(ymd_hms('1980-1-1 00:00:00'), ymd_hms('2021-12-31 21:00:00'), by = '3 hours')
dates_merra <- ts_merra %>% as.Date %>% unique

```


# impacts: data

## FEMA disaster declarations

### disaster declarations by county

```{r}
## get number of data points from FEMA API
h <- handle_setopt(new_handle())
api_call <- paste0(
  'https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries?',
  '$inlinecount=allpages&$top=1&$filter=state%20eq%20%27CA%27&')
api <- curl_download(api_call, tempfile(), handle = h)
n <- fromJSON(api)$metadata$count  #should be around 1,500

## get dataset from FEMA API
cl <- makeCluster(cores)
registerDoSNOW(cl)
pb <- txtProgressBar(min = 0, max = floor(n/1000), style = 3)
disasters <-
  foreach (
    i = 0:(n/1000),
    .combine = 'rbind', .packages = c('httr', 'curl', 'jsonlite'),
    .options.snow = opts) %dopar% {
      h <- handle_setopt(new_handle())
      api_call <- paste0(
        'https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries?',
        '$skip=', i*1000,
        '&$filter=state%20eq%20%27CA%27')
      api <- curl_download(api_call, tempfile(), handle = h)
      fromJSON(api)$DisasterDeclarationsSummaries
    }
stopCluster(cl)

```

### disaster declarations by number

```{r}
## get number of data points from FEMA API
h <- handle_setopt(new_handle())
api_call <- paste0(
  'https://www.fema.gov/api/open/v1/FemaWebDisasterDeclarations?',
  '$inlinecount=allpages&$top=1&$filter=stateCode%20eq%20%27CA%27&')
api <- curl_download(api_call, tempfile(), handle = h)
n <- fromJSON(api)$metadata$count  #should be around 350

## get dataset from FEMA API
cl <- makeCluster(cores)
registerDoSNOW(cl)
pb <- txtProgressBar(min = 0, max = max(c(1, floor(n/1000))), style = 3)
disasters_unique <-
  foreach (
    i = 0:(n/1000),
    .combine = 'rbind', .packages = c('httr', 'curl', 'jsonlite'),
    .options.snow = opts) %dopar% {
      h <- handle_setopt(new_handle())
      api_call <- paste0(
        'https://www.fema.gov/api/open/v1/FemaWebDisasterDeclarations?',
        '$skip=', i*1000,
        '&$filter=stateCode%20eq%20%27CA%27')
      api <- curl_download(api_call, tempfile(), handle = h)
      fromJSON(api)$FemaWebDisasterDeclarations
    }
stopCluster(cl)

```

### process downloaded data

```{r}
## subset to flood-related disasters
flood <- c('Coastal Storm', 'Dam/Levee Break', 'Flood', 'Severe Storm(s)')
floodnums <- disasters_unique %>%
  filter(incidentType %in% flood) %>%
  filter(declarationType == 'Major Disaster') %>%
  pull(disasterNumber)

## clean up disaster dataframes
disasters <- disasters %>%
  filter(disasterNumber %in% floodnums) %>%
  transmute(
    disasterNumber,
    fips = 1e3*toNumber(fipsStateCode) + toNumber(fipsCountyCode),
    incidentType, declarationType, declarationTitle,
    start_day = as.Date(ymd_hms(incidentBeginDate)),
    end_day = as.Date(ymd_hms(incidentEndDate)),
    iaProgramDeclared, ihProgramDeclared, paProgramDeclared, hmProgramDeclared) %>%
  right_join(
    california %>% transmute(fips = toNumber(GEOID), county = NAME), .,
    by = 'fips') %>%
  arrange(disasterNumber) %>%
  st_drop_geometry
disasters_unique <- disasters_unique %>%
  filter(disasterNumber %in% floodnums) %>%
  transmute(
    disasterNumber, incidentType,
    start_day = as.Date(ymd_hms(incidentBeginDate)),
    end_day = as.Date(ymd_hms(incidentEndDate))) %>%
  arrange(disasterNumber)

```

### checkpoint

```{r}
## save out
save(disasters, disasters_unique, floodnums, file = '_data/impacts/files/disasters_0922.Rdata')

# ## load from file
# load('_data/impacts/files/disasters_0922.Rdata')

```

## FEMA public assistance

```{r}
## get number of data points from FEMA API
h <- handle_setopt(new_handle())
api_call <- paste0(
  'https://www.fema.gov/api/open/v1/PublicAssistanceFundedProjectsDetails?',
  '$inlinecount=allpages&$top=1&$filter=state%20eq%20%27California%27&')
api <- curl_download(api_call, tempfile(), handle = h)
n <- fromJSON(api)$metadata$count  #should be around 19,000

## get dataset from FEMA API
cl <- makeCluster(cores)
registerDoSNOW(cl)
pb <- txtProgressBar(min = 0, max = floor(n/1000), style = 3)
pa_proj_details <-
  foreach (
    i = 0:(n/1000),
    .combine = 'rbind', .packages = c('httr', 'curl', 'jsonlite'),
    .options.snow = opts) %dopar% {
      h <- handle_setopt(new_handle())
      api_call <- paste0(
        'https://www.fema.gov/api/open/v1/PublicAssistanceFundedProjectsDetails?',
        '$skip=', i*1000,
        '&$filter=state%20eq%20%27California%27')
      api <- curl_download(api_call, tempfile(), handle = h)
      fromJSON(api)$PublicAssistanceFundedProjectsDetails
    }
stopCluster(cl)

```

### process downloaded data

```{r}
## clean up PA dataframe
pa <- pa_proj_details %>%
  filter(disasterNumber %in% floodnums) %>%
  transmute(
    disasterNumber, incidentType,
    pwNumber, dcc, damageCategory,
    fips = 6e3 + toNumber(countyCode),
    projectAmount, federalShareObligated, totalObligated) %>%
  right_join(
    california %>% transmute(fips = toNumber(GEOID), county = NAME), .,
    by = 'fips') %>%
  filter(!is.na(county)) %>%
  left_join(
    disasters %>%
      group_by(disasterNumber, start_day, end_day) %>%
      summarize(across(everything(), ~.x[1]), .groups = 'drop') %>%
      dplyr::select(disasterNumber, start_day, end_day),
    by = 'disasterNumber')

# ## note: PA data only exists for events after 2003
# pa %>% pull(disasterNumber) %>% unique %>% length
# disasters_unique %>% filter(start_day >= min(pa$start_day)) %>% nrow

```

```{r}
#### create table of PA funding by date & grid cell

## create pa_grid
pb <- txtProgressBar(min = 0, max = nrow(pa), style = 3)
cl <- makeCluster(cores)
registerDoSNOW(cl)
pa_grid <-
  foreach(
    j = 1:nrow(pa),
    .packages = c('sf', 'raster', 'lubridate', 'dplyr'),
    .inorder = FALSE,
    .options.snow = opts,
    .combine = 'rbind') %dopar% {
      cell_vals <-
        rasterize(pa[j,], grid_ca, getCover = TRUE)[]
      cell_ids <- which(cell_vals > 0.01)
      # if (length(cell_ids)==0) warning(paste0(j, ' has no spatial match'))
      date_ids <- which(dates_merra %within% interval(pa$start_day[j], pa$end_day[j]))
      expand.grid(
        cell = intersect(cell_ids, index_ca),
        date = dates_merra[date_ids],
        pa = pa$projectAmount[j]) %>%
        left_join(data.frame(cell = cell_ids, vals = cell_vals[cell_ids]), by = 'cell') %>%
        mutate(pa_pct = vals/sum(vals)) %>%
        select(-vals)
    }
stopCluster(cl)

## split damages over event space/time and combine overlapping events
pa_grid <-
  pa_grid %>%
  group_by(cell,date) %>%
  summarize(pa_amt = sum(pa*pa_pct), .groups = 'drop')
Sum(pa$projectAmount) == Sum(pa_grid$pa_amt)

```

### adjust to 2022 dollars

```{r}
pa_grid <- pa_grid %>% 
  mutate(mo = month(date), yr = year(date)) %>% 
  left_join(inflation, by = c('mo', 'yr')) %>%
  mutate(pa_amt = pa_amt*inflation) %>% 
  select(cell, date, pa_amt)

```

### checkpoint 

```{r}
## save out
save(pa, pa_grid, file = '_data/impacts/files/pa_0922.Rdata')

# ## load from file
# load('_data/impacts/files/pa_0922.Rdata')

```

## state disaster declarations

https://www.ftb.ca.gov/file/business/deductions/disaster-codes.html
https://dot.ca.gov/programs/local-assistance/fed-and-state-programs/emergency-relief-program/declarations
https://www.federalregister.gov/documents/2019/06/18/2019-12852/california-major-disaster-and-related-determinations
https://www.ca.gov/archive/gov39/2017/02/page/4/index.html

conclusion: not a good dataset, because they do not have physically relevant end dates

## NCEI storm event database

### download data 

```{r}
## set up links & file specifications
url <- 'https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/'
links <- read_html(url) %>% html_elements('a') %>% html_text()
links <- links[grepl('StormEvents_details', links)]

colspecs <- cols(
  BEGIN_YEARMONTH = col_skip(),
  BEGIN_DAY = col_skip(),
  BEGIN_TIME = col_skip(),
  END_YEARMONTH = col_skip(),
  END_DAY = col_skip(),
  END_TIME = col_skip(),
  EPISODE_ID = col_double(),
  EVENT_ID = col_double(),
  STATE = col_character(),
  STATE_FIPS = col_skip(),
  YEAR = col_skip(),
  MONTH_NAME = col_skip(),
  EVENT_TYPE = col_character(),
  CZ_TYPE = col_character(),
  CZ_FIPS = col_double(),
  CZ_NAME = col_character(),
  WFO = col_skip(),
  BEGIN_DATE_TIME = col_character(),
  CZ_TIMEZONE = col_character(),
  END_DATE_TIME = col_character(),
  INJURIES_DIRECT = col_double(),
  INJURIES_INDIRECT = col_double(),
  DEATHS_DIRECT = col_double(),
  DEATHS_INDIRECT = col_double(),
  DAMAGE_PROPERTY = col_character(),
  DAMAGE_CROPS = col_skip(),
  SOURCE = col_skip(),
  MAGNITUDE = col_skip(),
  MAGNITUDE_TYPE = col_skip(),
  FLOOD_CAUSE = col_skip(),
  CATEGORY = col_skip(),
  TOR_F_SCALE = col_skip(),
  TOR_LENGTH = col_skip(),
  TOR_WIDTH = col_skip(),
  TOR_OTHER_WFO = col_skip(),
  TOR_OTHER_CZ_STATE = col_skip(),
  TOR_OTHER_CZ_FIPS = col_skip(),
  TOR_OTHER_CZ_NAME = col_skip(),
  BEGIN_RANGE = col_skip(),
  BEGIN_AZIMUTH = col_skip(),
  BEGIN_LOCATION = col_skip(),
  END_RANGE = col_skip(),
  END_AZIMUTH = col_skip(),
  END_LOCATION = col_skip(),
  BEGIN_LAT = col_double(),
  BEGIN_LON = col_double(),
  END_LAT = col_double(),
  END_LON = col_double(),
  EPISODE_NARRATIVE = col_skip(),
  EVENT_NARRATIVE = col_skip(),
  DATA_SOURCE = col_skip())

hazards <-
  c('Blizzard', #'Hail', 'Thunderstorm Wind',
    'Flash Flood', 'Flood', 'Heavy Rain', 'Heavy Snow',
    'Winter Storm', 'Winter Weather')

```

```{r}
#### option 1: save to file, then read from file

# ## download & unzip
# temp <- tempdir()
# pb <- txtProgressBar(min = 0, max = length(links), style = 3)
# for (i in 1:length(links)) {
#   download.file(
#     url = paste0(url, links[i]), 
#     destfile = paste0(temp, '\\', links[i]),
#     quiet = TRUE)
#   gunzip(
#     filename = paste0(temp, '\\', links[i]), 
#     destname = paste0('_data/NCEI/', gsub('.gz', '', links[i])), 
#     skip = TRUE)
#   setTxtProgressBar(pb, i)
# }
## combine into one file
ncei <-
  foreach (yr = 1980:2020, .combine = 'rbind') %do% {
    list.files('D:/Research/_data/NCEI', full.names = TRUE) %>%
      .[grep(paste0('d',yr), .)] %>%
      read_csv(col_types = colspecs) %>%
      filter(STATE == 'CALIFORNIA' & EVENT_TYPE %in% hazards)
  }

#### option 2: download straight from web

# start <- Sys.time()
# pb <- setTxtProgressBar(min = 0, max = length(links), style = 3)
# ncei <-
#   foreach (
#     i = 1:length(links), 
#     .packages = c('dplyr', 'readr'),
#     .combine = 'rbind',
#     .options.snow = list(progress = progress)) %do% {
#     read_csv(paste0(url, links[i]), col_types = colspecs) %>% 
#       filter(STATE == 'CALIFORNIA' & EVENT_TYPE %in% hazards)
#     }
# Sys.time() - start

```

### process downloaded data

```{r}
## format ncei damage as number
# ncei %>% 
#   filter(!grepl('K', DAMAGE_PROPERTY)) %>% 
#   filter(!grepl('M', DAMAGE_PROPERTY)) %>% 
#   pull(DAMAGE_PROPERTY) %>% unique

ncei <- ncei %>%
  mutate(
    BEGIN_DATE_TIME = dmy_hms(BEGIN_DATE_TIME),
    END_DATE_TIME = dmy_hms(END_DATE_TIME),
    DAMAGE_VAL = case_when(
      grepl('K',DAMAGE_PROPERTY) ~ toNumber(gsub('K','',DAMAGE_PROPERTY))*1e3,
      grepl('M',DAMAGE_PROPERTY) ~ toNumber(gsub('M','',DAMAGE_PROPERTY))*1e6,
      grepl('B',DAMAGE_PROPERTY) ~ toNumber(gsub('B','',DAMAGE_PROPERTY))*1e9,
      TRUE ~ 0)) %>%
  filter(year(BEGIN_DATE_TIME) >= 1996)

# sum.na(ncei$DAMAGE_VAL)
# min(ncei$DAMAGE_VAL)

```

```{r}
#### split ncei into zone & county geometries

## download NWS warning zones
# https://www.weather.gov/gis/PublicZones

zones1 <- st_read('D:/Research/_data/NWS/forecast zones/z_16mr01.shp') %>% 
  filter(STATE == 'CA') %>%
  st_set_crs(nad83) %>% st_transform(st_crs(california)) %>%
  transmute(ZONE = toNumber(gsub('CA', '', STATE_ZONE)), NAME = str_to_upper(NAME))
zones2 <- st_read('D:/Research/_data/NWS/forecast zones/z_10nv20.shp') %>% 
  filter(STATE == 'CA') %>%
  st_transform(st_crs(california)) %>%
  transmute(ZONE = toNumber(gsub('CA', '', STATE_ZONE)), NAME = str_to_upper(NAME))
zones3 <- st_read('D:/Research/_data/NWS/forecast zones/z_08mr23.shp') %>% 
  filter(STATE == 'CA') %>%
  st_transform(st_crs(california)) %>%
  transmute(ZONE = toNumber(gsub('CA', '', STATE_ZONE)), NAME = str_to_upper(NAME))

zones <- zones1 %>% rbind(zones2 %>% filter(!(ZONE %in% zones1$ZONE)))
zones <- zones %>% rbind(zones3 %>% filter(!(ZONE %in% zones$ZONE)))

## subset zones
# note: about 2% of the zones don't match up
ncei_z <- ncei %>% 
  filter(CZ_TYPE == 'Z') %>%
  inner_join(zones, ., by = c('ZONE' = 'CZ_FIPS'))

## subset counties
ncei_c <- ncei %>%
  filter(CZ_TYPE == 'C') %>%
  inner_join(
    california %>% transmute(COUNTYFP = toNumber(COUNTYFP), NAME), .,
    by = c('COUNTYFP' = 'CZ_FIPS'))

```

### create table of NCEI storm events by date & grid cell

```{r}
## NCEI county events
pb <- txtProgressBar(min = 0, max = nrow(ncei_c), style = 3)
cl <- makeCluster(cores)
registerDoSNOW(cl)
ncei_c_grid <-
  foreach(
    j = 1:nrow(ncei_c),
    .packages = c('sf', 'raster', 'lubridate', 'dplyr'),
    .inorder = FALSE,
    .options.snow = opts,
    .combine = 'rbind') %dopar% {
      cell_vals <-
        rasterize(ncei_c[j,], grid_ca, getCover = TRUE)[]
      cell_ids <- which(cell_vals > 0.01)
      date_ids <-
        which(dates_merra %within%
                interval(as.Date(ncei_c$BEGIN_DATE_TIME[j]), as.Date(ncei_c$END_DATE_TIME[j])))
      expand.grid(
        cell = intersect(cell_ids, index_ca),
        date = dates_merra[date_ids],
        ncei_damage = ncei_c$DAMAGE_VAL[j]) %>%
        left_join(data.frame(cell = cell_ids, vals = cell_vals[cell_ids]), by = 'cell') %>%
        mutate(ncei_pct = vals/sum(vals)) %>%
        select(-vals)
    }
cat('\n')

## NCEI zone events
pb <- txtProgressBar(min = 0, max = nrow(ncei_z), style = 3)
ncei_z_grid <-
  foreach(
    j = 1:nrow(ncei_z),
    .packages = c('sf', 'raster', 'lubridate', 'dplyr'),
    .inorder = FALSE,
    .options.snow = opts,
    .combine = 'rbind') %dopar% {
      cell_vals <-
        rasterize(ncei_z[j,], grid_ca, getCover = TRUE)[]
      cell_ids <- which(cell_vals > 0.01)
      date_ids <-
        which(dates_merra %within%
                interval(as.Date(ncei_z$BEGIN_DATE_TIME[j]), as.Date(ncei_z$END_DATE_TIME[j])))
      expand.grid(
        cell = intersect(cell_ids, index_ca),
        date = dates_merra[date_ids],
        ncei_damage = ncei_z$DAMAGE_VAL[j]) %>%
        left_join(data.frame(cell = cell_ids, vals = cell_vals[cell_ids]), by = 'cell') %>%
        mutate(ncei_pct = vals/sum(vals)) %>%
        select(-vals)
    }
stopCluster(cl)
cat('\n')

## combine zones & counties
ncei_grid <- rbind(ncei_c_grid, ncei_z_grid)

## split damages over event space/time and combine overlapping events
ncei_grid <- ncei_grid %>%
  group_by(cell,date) %>%
  summarize(
    ncei_damage = sum(ncei_damage * ncei_pct),
    .groups = 'drop')
Sum(ncei_c$DAMAGE_VAL) + Sum(ncei_z$DAMAGE_VAL) == Sum(ncei_grid$ncei_damage)

```

### adjust to 2022 dollars

```{r}
ncei_grid <- ncei_grid %>% 
  mutate(mo = month(date), yr = year(date)) %>% 
  left_join(inflation, by = c('mo', 'yr')) %>% 
  mutate(ncei_damage = ncei_damage * inflation) %>% 
  select(cell, date, ncei_damage) %>% 
  mutate(ncei_event = TRUE)

```

### checkpoint

```{r}
## save out
save(ncei, ncei_grid, file = 'D:/2-sequences/_data/impacts/ncei_0222.Rdata')

# ## load from file
# load('_data/impacts/files/ncei_0209.Rdata')

```



## NFIP

### download claims

```{r}
## get number of claims from FEMA API
h <- handle_setopt(new_handle())
api_call <- paste0(
  'https://www.fema.gov/api/open/v1/FimaNfipClaims?',
  '$inlinecount=allpages&', '$top=1&', '$filter=state%20eq%20%27CA%27&')
api <- curl_download(api_call, tempfile(), handle = h)
n <- fromJSON(api)$metadata$count  #should be around 50,000

## get claims dataset from FEMA API
pb <- txtProgressBar(min = 0, max = n/1000, style = 3)
cl <- makeCluster(cores)
registerDoSNOW(cl)
claims <-
  foreach (
    i = 0:(n/1000),
    .combine = 'rbind', .packages = c('httr', 'curl', 'jsonlite'),
    .options.snow = opts) %dopar% {
      h <- handle_setopt(new_handle())
      api_call <- paste0(
        'https://www.fema.gov/api/open/v1/FimaNfipClaims?',
        '$skip=', i*1000,
        '&$filter=state%20eq%20%27CA%27')
      api <- curl_download(api_call, tempfile(), handle = h)
      fromJSON(api)$FimaNfipClaims
  }
stopCluster(cl)

# claims_save <- claims

```

### download policies 

```{r}
## get number of policies from FEMA API
h <- handle_setopt(new_handle())
api_call <- paste0(
  'https://www.fema.gov/api/open/v1/FimaNfipPolicies?',
  '$inlinecount=allpages&', '$top=1&', '$filter=propertyState%20eq%20%27CA%27&')
api <- curl_download(api_call, tempfile(), handle = h)
n <- fromJSON(api)$metadata$count  #should be around 3.3 million

## get policies dataset from FEMA API
options(scipen = 8)
pb <- txtProgressBar(min = 0, max = n/1000, style = 3)
cl <- makeCluster(8)
registerDoSNOW(cl)
policies <-
  foreach (
    i = 0:(n/1000),
    .combine = 'rbind',
    .packages = c('httr', 'curl', 'jsonlite'),
    .options.snow = opts) %dopar% {
      try({
        h <- handle_setopt(new_handle())
        api_call <- paste0(
          'https://www.fema.gov/api/open/v1/FimaNfipPolicies?',
          '$skip=', i*1000,
          '&$filter=propertyState%20eq%20%27CA%27')
        api <- curl_download(api_call, tempfile(), handle = h)
        fromJSON(api)$FimaNfipPolicies
      }, silent = TRUE)
  }
stopCluster(cl)

# for (ii in which(lapply(policies,class) == 'try-error')) {
#   i <- ii-1
#   h <- handle_setopt(new_handle())
#   api_call <- paste0(
#     'https://www.fema.gov/api/open/v1/FimaNfipPolicies?',
#     '$skip=', i*1000,
#     '&$filter=propertyState%20eq%20%27CA%27')
#   api <- curl_download(api_call, tempfile(), handle = h)
#   policies[[ii]] <- fromJSON(api)$FimaNfipPolicies
# }
# policies <- policies %>% reduce(rbind)

## checkpoint
# policies_save <- policies
save(policies, file = '_data/impacts/policies_save.Rdata')

# load('_data/impacts/policies_save.Rdata')

```

### adjust to 2022 dollars 

```{r}
## summarize claims
claims_grid <- claims %>%
  filter(!is.na(longitude) & !is.na(latitude)) %>%
  filter(toNumber(longitude) < -110) %>%
  st_as_sf(coords = c('longitude', 'latitude'), crs = nad83) %>%
  st_transform(crs(grid_ca)) %>%
  mutate(cell = raster::extract(grid_ca, as_Spatial(.))) %>%
  filter(!is.na(cell)) %>%
  st_drop_geometry %>%
  group_by(cell, date = as.Date(dateOfLoss)) %>%
  summarize(
    claims_num = length(dateOfLoss),
    claims_value =
      Sum(amountPaidOnBuildingClaim) + Sum(amountPaidOnContentsClaim),
    claims_coverage =
      Sum(totalBuildingInsuranceCoverage) + Sum(totalContentsInsuranceCoverage),
    .groups = 'drop') %>%
  mutate(yr = year(date), mo = month(date)) %>%
  left_join(inflation, by = c('yr', 'mo')) %>%
  mutate(
    claims_value = claims_value * inflation,
    claims_coverage = claims_coverage * inflation,
    claims_dr = claims_value / claims_coverage) %>%
  select(-inflation, -yr, -mo)

## summarize policies
policies_grid <- policies %>%
  mutate(longitude = toNumber(longitude), latitude = toNumber(latitude)) %>% 
  filter(!is.na(longitude) & !is.na(latitude)) %>%
  st_as_sf(coords = c('longitude', 'latitude'), crs = nad83) %>%
  st_transform(crs(grid_ca)) %>%
  mutate(cell = raster::extract(grid_ca, as_Spatial(.))) %>%
  filter(!is.na(cell)) %>%
  st_drop_geometry %>%
  mutate(start = ymd_hms(policyEffectiveDate)) %>%
  group_by(cell, mo = month(policyEffectiveDate), yr = year(policyEffectiveDate)) %>%
  summarize(
    policies_num = length(start),
    policies_value =
      Sum(toNumber(totalBuildingInsuranceCoverage)) + 
      Sum(toNumber(totalContentsInsuranceCoverage)),
    .groups = 'drop') %>%
  filter(yr < 2022) %>%
  mutate(policies_value = toNumber(policies_value)) %>%
  mutate(policies_value = case_when(!is.na(policies_value) ~ policies_value, TRUE ~ 0)) %>%
  left_join(inflation, by = c('yr', 'mo')) %>%
  mutate(policies_value_adj = policies_value * inflation) %>%
  group_by(cell, coverage_wy = yr + ifelse(mo %in% 4:12, 1, 0)) %>%
  summarize(across(c(policies_num, policies_value_adj), Sum), .groups = 'drop')

# ## clean up
# rm(bls, policies_save, claims_save)

```

### checkpoint 

```{r}
## save out
save(claims, claims_grid, policies, policies_grid, file = '_data/impacts/NFIP_0210.Rdata')

# ## load from file
# load('_data/impacts/NFIP_0210.Rdata')

```


## population

### load population data

```{r}
#### load population data by census tract & decade-ish

## 2000
tracts2000 <- tracts(year = 2000, state = 'CA')
pop2000 <- getCensus(
    name = "dec/sf1", vars = "P001001", vintage = 2000,
    region = "tract:*", regionin = 'state:06') %>%
  mutate(tract = case_when(str_length(tract)==4 ~ toNumber(tract)*100, TRUE ~ toNumber(tract))) %>%
  transmute(
    pop = P001001,
    fips = toNumber(state)*1e9 + toNumber(county)*1e6 + toNumber(tract)) %>%
  right_join(tracts2000 %>% transmute(fips = toNumber(CTIDFP00)), ., by = 'fips')

## 2010
tracts2010 <- tracts(year = 2010, state = 'CA')
pop2010_dec <- getCensus(
    name = "dec/sf1", vars = "P001001", vintage = 2010,
    region = "tract:*", regionin = 'state:06')
pop2010_acs <- getCensus(
  name = 'acs/acs5', vars = 'B01003_001E', vintage = 2010,
  region = 'tract:*', regionin = 'state:06')
pop2010 <-
  full_join(
    pop2010_dec %>%
      mutate(tract = case_when(str_length(tract)==4 ~ toNumber(tract)*100, TRUE ~ toNumber(tract))) %>%
      transmute(
        dec = P001001,
        fips = toNumber(state)*1e9 + toNumber(county)*1e6 + toNumber(tract)),
    pop2010_acs %>%
      transmute(
        acs = B01003_001E,
        fips = toNumber(state)*1e9 + toNumber(county)*1e6 + toNumber(tract)),
    by = 'fips') %>%
  right_join(tracts2010 %>% transmute(fips = toNumber(GEOID10)), ., by = 'fips')

# ggplot(pop2010) +
#   geom_point(aes(x = acs, y = dec)) +
#   scale_x_origin() + scale_y_origin() + geom_parity() + coord_fixed()

## 2015
tracts2015 <- tracts(year = 2015, state = 'CA')
pop2015 <- getCensus(
  name = 'acs/acs5', vars = 'B01003_001E', vintage = 2015,
  region = 'tract:*', regionin = 'state:06') %>%
  transmute(
    pop = B01003_001E,
    fips = toNumber(state)*1e9 + toNumber(county)*1e6 + toNumber(tract)) %>%
  full_join(tracts2015 %>% transmute(fips = toNumber(GEOID), temp = 1), ., by = 'fips')

## 2020
tracts2020 <- tracts(year = 2020, state = 'CA')
pop2020 <- getCensus(
  name = 'acs/acs5', vars = 'B01003_001E', vintage = 2020,
  region = 'tract:*', regionin = 'state:06') %>%
  transmute(
    pop = B01003_001E,
    fips = toNumber(state)*1e9 + toNumber(county)*1e6 + toNumber(tract)) %>%
  right_join(tracts2020 %>% transmute(fips = toNumber(GEOID)), ., by = 'fips')

```

```{r}
## rasterize polygons
pop2000_raster <- gridpop(pop2000, grid_ca, fact = 10)
pop2010_raster <- 
  gridpop(pop2010 %>% rename(pop = dec), grid_ca, fact = 10)
pop2015_raster <- gridpop(pop2015, grid_ca, fact = 10)
pop2020_raster <- gridpop(pop2020, grid_ca,fact = 10)

## clean up
rm(pop2000, tracts2000, pop2010, tracts2010, pop2015, tracts2015, pop2020, tracts2020)

```

### checkpoint

```{r}
## save out
save(pop2000_raster, pop2010_raster, pop2015_raster, pop2020_raster,
     file = '_data/impacts/files/poprasters_0922.Rdata')

# ## load from file
# load('_data/impacts/files/poprasters_0922.Rdata')

```

### plot rasters

```{r}
# ## plot results
# ggplot() + 
#   geom_raster(
#     data = raster.df(pop2000_raster) %>% filter(value>0),
#     aes(x=x, y=y, fill=value)) + 
#   geom_sf(data = wus %>% filter(STUSPS == 'CA'), fill = NA) + 
#   scale_fill_scico(
#     '2000 \nGridded \nPopulation \n(millions)',
#     palette = 'davos', direction = -1,
#     labels = comma_format(scale = 1e-6), limits = c(0, 5.5e6)) + 
#   theme_void() + theme(legend.position = c(0.9,0.7))
# ggplot() + 
#   geom_raster(
#     data = raster.df(pop2010_raster) %>% filter(value>0),
#     aes(x=x, y=y, fill=value)) + 
#   geom_sf(data = wus %>% filter(STUSPS == 'CA'), fill = NA) + 
#   scale_fill_scico(
#     '2010 \nGridded \nPopulation \n(millions)',
#     palette = 'davos', direction = -1,
#     labels = comma_format(scale = 1e-6), limits = c(0, 5.5e6)) + 
#   theme_void() + theme(legend.position = c(0.9,0.7))
# ggplot() + 
#   geom_raster(
#     data = raster.df(pop2015_raster) %>% filter(value>0),
#     aes(x=x, y=y, fill=value)) + 
#   geom_sf(data = wus %>% filter(STUSPS == 'CA'), fill = NA) + 
#   scale_fill_scico(
#     '2015 \nGridded \nPopulation \n(millions)',
#     palette = 'davos', direction = -1,
#     labels = comma_format(scale = 1e-6), limits = c(0, 5.5e6)) + 
#   theme_void() + theme(legend.position = c(0.9,0.7))
# ggplot() + 
#   geom_raster(
#     data = raster.df(pop2020_raster) %>% filter(value>0),
#     aes(x=x, y=y, fill=value)) + 
#   geom_sf(data = wus %>% filter(STUSPS == 'CA'), fill = NA) +
#   scale_fill_scico(
#     '2020 \nGridded \nPopulation \n(millions)',
#     palette = 'davos', direction = -1,
#     labels = comma_format(scale = 1e-6), limits = c(0, 5.5e6)) + 
#   theme_void() + theme(legend.position = c(0.9,0.7))

```

## NWS WWA

### load WWA files by year

```{r}
# https://mesonet.agron.iastate.edu/request/gis/watchwarn.phtml
# https://github.com/akrherz/pyIEM/blob/main/src/pyiem/nws/vtec.py

events <- c('WS','SV','FF','FA','FL','WI','WW')  # matches previous work
sf::sf_use_s2(FALSE)

wwa <- map_dfr(
  .x = 1986:2021,
  .f = function(yr) {
    paste0('C:/Users/cbowers/Downloads/wwa/wwa_', yr, '01010000_', yr, '12312359.shp') %>%
      st_read(quiet = TRUE) %>%
      st_transform(st_crs(wus)) %>%
      .[wus %>% filter(STUSPS == 'CA'),] %>%
      transmute(
        issued = ymd_hm(ISSUED),
        expired = ymd_hm(EXPIRED),
        phenom = PHENOM,
        vtec = factor(case_when(
          SIG=='W' ~ 'warning',
          SIG=='Y' ~ 'advisory',
          SIG=='A' ~ 'watch',
          TRUE ~ 'other'))) %>%
      filter(phenom %in% events) %>%
      filter(vtec != 'other') %>%
      mutate(year = yr)
  })
## takes about twenty minutes to create this object

```

### create table of WWA by date & grid cell

```{r}
pb <- txtProgressBar(min = 0, max = nrow(wwa), style = 3)
cl <- makeCluster(cores)
registerDoSNOW(cl)
wwa_grid <-
  foreach(
    j = 1:nrow(wwa),
    .packages = c('sf', 'raster', 'lubridate', 'dplyr'),
    .inorder = FALSE,
    .options.snow = opts,
    .combine = 'rbind') %dopar% {
      cell_ids <- which(rasterize(wwa[j,], grid_ca, getCover = TRUE)[] > 0.25)
      date_ids <-
        which(dates_merra %within% interval(as.Date(wwa$issued[j]), as.Date(wwa$expired[j])))
      expand.grid(
        cell = intersect(cell_ids, index_ca),
        date = dates_merra[date_ids],
        phenom = wwa$phenom[j],
        vtec = paste(wwa$vtec[j]))
    }
stopCluster(cl)
## takes another ten minutes

## check for multiple overlapping wwa events
wwa_grid <- wwa_grid %>%
  group_by(cell, date) %>%
  summarize(wwa_phenom = mode(phenom), wwa_vtec = mode(vtec), .groups = 'drop')

```

### checkpoint

```{r}
## save out
save(wwa, wwa_grid, file = '_data/impacts/files/wwa_0922.Rdata')

# ## load from file
# load('_data/impacts/files/wwa_0922.Rdata')

```

